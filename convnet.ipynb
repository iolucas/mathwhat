{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/iolucas/dlnd-projects/blob/master/image-classification/dlnd_image_classification.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from data_manager import load_dataset\n",
    "\n",
    "import json\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "import random\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StrokesDataset:\n",
    "    def __init__(self, db_addr=\"localhost\"):\n",
    "        #Connect to db\n",
    "        self.conn = psycopg2.connect(\"dbname='postgres' user='postgres' host='{}' password='lucas'\".format(db_addr))\n",
    "        self.cur = self.conn.cursor()\n",
    "        \n",
    "        #Retrieve and cache ids list to use to query data later\n",
    "        self.cur.execute(\"SELECT id FROM samples LIMIT 300000\")\n",
    "        id_list = self.cur.fetchall()\n",
    "        self.id_cache = [id_value[0] for id_value in id_list]\n",
    "        \n",
    "        #Lets save key_to_ind to ensure the labels will be in the same order all the time\n",
    "        #Try to load key_to_ind, if not succeed, get it and save it\n",
    "        try:\n",
    "            key_list = pickle.load(open(\"key_list.pickle\", \"rb\"))\n",
    "            #self.key_to_ind = pickle.load(open(\"key_to_ind.pickle\", \"rb\"))\n",
    "            print(\"Loaded key_list file.\")\n",
    "        except:\n",
    "            #Retrieve and cache labels\n",
    "            self.cur.execute(\"SELECT DISTINCT key FROM samples LIMIT 300000\")\n",
    "            key_list = [key_value[0] for key_value in self.cur.fetchall()]\n",
    "            pickle.dump(key_list, open(\"key_list.pickle\", \"wb\"), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            print(\"Generated new key_list file.\")\n",
    "            \n",
    "        self.key_to_ind = dict([(key, ind) for ind, key in enumerate(key_list)])\n",
    "        self.ind_to_key = key_list\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "    def get_train_valid_test_ids(self):\n",
    "        train_ids, valid_ids = train_test_split(self.id_cache, test_size=0.4)\n",
    "        valid_ids, test_ids = train_test_split(valid_ids, test_size=0.5)\n",
    "        return tuple(train_ids), tuple(valid_ids), tuple(test_ids)\n",
    "    \n",
    "    def DEPRECTED_get_batches(self, ids, batch_size):\n",
    "        assert batch_size < 5000\n",
    "        \n",
    "        self.cur.execute(\"SELECT key, strokes FROM samples WHERE id in {}\".format(ids))\n",
    "       \n",
    "        while True:\n",
    "            data_batch = self.cur.fetchmany(batch_size)\n",
    "            if len(data_batch) == 0:\n",
    "                break\n",
    "                \n",
    "            ziped = list(zip(*data_batch)) #Use zip to separate labels from datapoints\n",
    "            \n",
    "            yield ziped[1], ziped[0]\n",
    "            \n",
    "    def get_batches(self, ids, batch_size):\n",
    "        assert batch_size < 5000\n",
    "        \n",
    "        for i in range(0,len(ids), batch_size):        \n",
    "            self.cur.execute(\"SELECT key, strokes FROM samples WHERE id in {}\".format(ids[i:i+batch_size]))\n",
    "       \n",
    "            data_batch = self.cur.fetchall()\n",
    "                \n",
    "            ziped = list(zip(*data_batch)) #Use zip to separate labels from datapoints\n",
    "            \n",
    "            yield ziped[1], ziped[0]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_image(strokes_list):\n",
    "    im = Image.new(mode=\"1\", size=(500,500))\n",
    "\n",
    "    draw = ImageDraw.Draw(im)\n",
    "    #draw.moveTo(strokes[0][:2], fill=128)\n",
    "    #print(strokes)\n",
    "    #draw.line((0, 0) + im.size, fill=128)\n",
    "    #draw.line((0, im.size[1], im.size[0], 0), fill=128)\n",
    "    for strokes in strokes_list:\n",
    "        draw.line(strokes, fill=128, width=5)\n",
    "    \n",
    "    \n",
    "    #for stroke in strokes[0:]:\n",
    "        #print(stroke.tolist())\n",
    "        #draw.line(stroke, fill=128, width=10)\n",
    "\n",
    "    plt.imshow(im)\n",
    "\n",
    "# write to stdout\n",
    "#im.save(sys.stdout, \"PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_tuples(strokes_list):\n",
    "    strokes_group = list()\n",
    "    for strokes in strokes_list:\n",
    "        strokes_tuples = list()\n",
    "        for x,y,t in strokes:\n",
    "            strokes_tuples.append((x,y))\n",
    "        strokes_group.append(strokes_tuples)\n",
    "    return strokes_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_random_tuple(size):\n",
    "    assert size < 100000\n",
    "    rand_list = list()\n",
    "    while len(rand_list) < size:\n",
    "        rand_value = random.randint(1,50000)\n",
    "        if rand_value not in rand_list:\n",
    "            rand_list.append(rand_value)\n",
    "        \n",
    "    return tuple(rand_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded key_list file.\n"
     ]
    }
   ],
   "source": [
    "ds = StrokesDataset(\"189.62.80.32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1097"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.key_to_ind[ds.ind_to_key[1097]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset, validset, testset = ds.get_train_valid_test_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.67 s\n",
      "upgreek-OT1-_uptau\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQsAAAD8CAYAAABgtYFHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc2d8c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%time test_batch = next(ds.get_batches(trainset,1000))\n",
    "draw_image(gen_tuples(test_batch[0][59]))\n",
    "print(test_batch[1][59])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 93, 105,   0,   1,   0],\n",
       "       [  1,   0,   1,   0,   0],\n",
       "       [  3,   0,   1,   0,   0],\n",
       "       [  2,  -1,   1,   0,   0],\n",
       "       [  4,  -1,   1,   0,   0],\n",
       "       [  4,   0,   1,   0,   0],\n",
       "       [  4,  -1,   1,   0,   0],\n",
       "       [  4,   0,   1,   0,   0],\n",
       "       [  2,   0,   1,   0,   0],\n",
       "       [  3,   0,   1,   0,   0],\n",
       "       [  1,   0,   1,   0,   0],\n",
       "       [  2,   0,   1,   0,   0],\n",
       "       [  2,   0,   1,   0,   0],\n",
       "       [  1,   0,   1,   0,   0],\n",
       "       [  2,   0,   1,   0,   0],\n",
       "       [  2,   0,   1,   0,   0],\n",
       "       [  1,   0,   1,   0,   0],\n",
       "       [  1,   0,   1,   0,   0],\n",
       "       [-37,  12,   0,   1,   0],\n",
       "       [  1,   0,   1,   0,   0],\n",
       "       [  3,   0,   1,   0,   0],\n",
       "       [  5,   0,   1,   0,   0],\n",
       "       [  4,   0,   1,   0,   0],\n",
       "       [  4,   0,   1,   0,   0],\n",
       "       [  4,   0,   1,   0,   0],\n",
       "       [  3,   0,   1,   0,   0],\n",
       "       [  2,   0,   1,   0,   0],\n",
       "       [  2,   0,   1,   0,   0],\n",
       "       [  2,   1,   1,   0,   0],\n",
       "       [  1,   0,   1,   0,   0],\n",
       "       [  1,   0,   1,   0,   0],\n",
       "       [  1,   0,   1,   0,   0],\n",
       "       [  1,   0,   1,   0,   0],\n",
       "       [  1,   0,   1,   0,   0],\n",
       "       [  2,   0,   1,   0,   0],\n",
       "       [  1,   0,   1,   0,   0],\n",
       "       [  1,   0,   1,   0,   0],\n",
       "       [  1,   0,   1,   0,   0],\n",
       "       [  1,   0,   1,   0,   0],\n",
       "       [ -5, -25,   0,   1,   0],\n",
       "       [ -1,   1,   1,   0,   0],\n",
       "       [ -1,   4,   1,   0,   0],\n",
       "       [ -1,   4,   1,   0,   0],\n",
       "       [ -2,   4,   1,   0,   0],\n",
       "       [ -1,   6,   1,   0,   0],\n",
       "       [ -2,   5,   1,   0,   0],\n",
       "       [  0,   3,   1,   0,   0],\n",
       "       [ -2,   5,   1,   0,   0],\n",
       "       [ -1,   3,   1,   0,   0],\n",
       "       [ -2,   4,   1,   0,   0],\n",
       "       [ -1,   2,   1,   0,   0],\n",
       "       [ -1,   2,   1,   0,   0],\n",
       "       [ -1,   1,   1,   0,   0],\n",
       "       [ -1,   1,   1,   0,   0],\n",
       "       [  0,   2,   1,   0,   0],\n",
       "       [ -1,   1,   1,   0,   0],\n",
       "       [  0,   1,   0,   0,   1]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform_data(strokes_list):\n",
    "    \"\"\"Convert strokes to match (deltaX, deltaY, is_drawing, is_moving, is_finished)\"\"\"\n",
    "    \n",
    "    onehots = list() #Gen pen moving onehots\n",
    "    flatten_strokes = [[0,0,0]] #Reshape strokes lists\n",
    "    \n",
    "    for strokes in strokes_list:\n",
    "        strokes = sorted(strokes, key=lambda a: a.__getitem__(2), reverse=False) #ensure strokes are in order\n",
    "        flatten_strokes += strokes\n",
    "        strokes_onehots = [[0,1,0]] + [[1,0,0]]*(len(strokes)-1)\n",
    "        onehots += strokes_onehots\n",
    "       \n",
    "    #Set end flag at the last coordinate\n",
    "    onehots[-1] = [0,0,1]\n",
    "        \n",
    "    flatten_strokes = np.array(flatten_strokes)\n",
    "    delta_strokes = flatten_strokes[1:,:2] - flatten_strokes[:-1,:2] #Get delta values (:2 removes the timing data)\n",
    "    \n",
    "    np_batch = np.concatenate((delta_strokes, onehots), axis=1)\n",
    "    \n",
    "    return np_batch.astype(int)\n",
    "    \n",
    "%time transform_data(test_batch[0][600])\n",
    "#draw_image(gen_tuples(test_batch[0][600]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/tutorials/recurrent_quickdraw<br>\n",
    "https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell<br>\n",
    "https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/DropoutWrapper<br>\n",
    "https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell<br>\n",
    "https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/stack_bidirectional_dynamic_rnn<br>\n",
    "https://github.com/tensorflow/models/blob/master/tutorials/rnn/quickdraw/train_model.py<br>\n",
    "https://github.com/udacity/deep-learning/blob/master/intro-to-rnns/Anna_KaRNNa_Solution.ipynb<br>\n",
    "https://github.com/tensorflow/magenta/tree/master/magenta/models/sketch_rnn<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _add_cudnn_rnn_layers(convolved, num_nodes, num_layers, keep_prob):\n",
    "    \"\"\"Adds CUDNN LSTM layers.\"\"\"\n",
    "    # Convolutions output [B, L, Ch], while CudnnLSTM is time-major.\n",
    "    convolved = tf.transpose(convolved, [1, 0, 2])\n",
    "    lstm = tf.contrib.cudnn_rnn.CudnnLSTM(\n",
    "        input_size=5,\n",
    "        num_layers=num_layers,\n",
    "        num_units=num_nodes,\n",
    "        dropout=keep_prob,\n",
    "        direction=\"bidirectional\")\n",
    "    outputs, _ = lstm(convolved)\n",
    "    # Convert back from time-major outputs to batch-major outputs.\n",
    "    outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_rnn_cell(lstm_size, keep_prob):\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    cell = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob=keep_prob)\n",
    "    return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#Create network\n",
    "lstm_size = 128\n",
    "num_layers = 2\n",
    "grad_clip = 9.0\n",
    "learning_rate = 0.01\n",
    "n_classes = len(ds.key_to_ind)\n",
    "\n",
    "inputs = tf.placeholder(tf.float32, shape=[None, None, 5], name=\"inputs\")\n",
    "keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "\n",
    "targets = tf.placeholder(tf.int32, shape=[None], name=\"targets\")\n",
    "\n",
    "targets_onehot = tf.one_hot(targets, n_classes)\n",
    "\n",
    "\n",
    "rnn_outputs = _add_cudnn_rnn_layers(inputs, lstm_size, num_layers, keep_prob)\n",
    "\n",
    "\n",
    "#fw_cells = [create_rnn_cell(lstm_size, keep_prob)] #Forwards cells\n",
    "#bw_cells = [create_rnn_cell(lstm_size, keep_prob)] #Backwards cells\n",
    "\n",
    "#rnn_outputs, output_state_fw, output_state_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n",
    "    #fw_cells, bw_cells, inputs, dtype=tf.float32)\n",
    "\n",
    "#Use [:,-1,:] to keep only the (batches, last outputs, classes)\n",
    "logits = tf.layers.dense(rnn_outputs[:,-1,:], n_classes)\n",
    "\n",
    "outputs = tf.nn.softmax(logits)\n",
    "\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(labels=targets_onehot, logits=logits)\n",
    "loss = tf.reduce_mean(loss)\n",
    "\n",
    "#Create optimizers\n",
    "#optimizer = tf.train.AdamOptimizer(0.01).minimize(cost)\n",
    "\n",
    "# Optimizer for training, using gradient clipping to control exploding gradients\n",
    "tvars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(targets_onehot, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_batches(dataset, batch_size):\n",
    "    for x_batch, y_batch in ds.get_batches(dataset, batch_size):\n",
    "        #Transform X data to get delta cordinates, \n",
    "        trans_x = list(map(transform_data, x_batch))\n",
    "        \n",
    "        #make them the same size of the max size        \n",
    "        max_size = max(map(len, trans_x))\n",
    "        x_data_buffer = np.array([0,0,0,0,1]) * np.ones((batch_size, max_size, 5)).astype(int)\n",
    "        for i, x_transf_data in enumerate(trans_x):\n",
    "            x_data_buffer[i][:len(x_transf_data)] = x_transf_data\n",
    "        \n",
    "        #transform y data to get indexes for each label\n",
    "        yield x_data_buffer, list(map(ds.key_to_ind.get, y_batch))\n",
    " \n",
    "#Test\n",
    "#%time testdata = next(get_batches(trainset, 5))\n",
    "#print(testdata[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%time draw_image(gen_tuples(next(test_batches)[0][8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126272"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.00336\n",
      "Working on accuracy...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [1000] vs. [91]\n\t [[Node: Equal = Equal[T=DT_INT64, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](ArgMax, ArgMax_1)]]\n\nCaused by op 'Equal', defined at:\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\tornado\\ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-244-8dc58c883e7d>\", line 41, in <module>\n    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(targets_onehot, 1))\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 672, in equal\n    result = _op_def_lib.apply_op(\"Equal\", x=x, y=y, name=name)\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Incompatible shapes: [1000] vs. [91]\n\t [[Node: Equal = Equal[T=DT_INT64, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](ArgMax, ArgMax_1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [1000] vs. [91]\n\t [[Node: Equal = Equal[T=DT_INT64, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](ArgMax, ArgMax_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-246-e6cbadfaf50a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m                 \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                 \u001b[0mtargets\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mY_valid\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m             })\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1050\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1052\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [1000] vs. [91]\n\t [[Node: Equal = Equal[T=DT_INT64, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](ArgMax, ArgMax_1)]]\n\nCaused by op 'Equal', defined at:\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\tornado\\ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-244-8dc58c883e7d>\", line 41, in <module>\n    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(targets_onehot, 1))\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 672, in equal\n    result = _op_def_lib.apply_op(\"Equal\", x=x, y=y, name=name)\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\du0x\\Anaconda2\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Incompatible shapes: [1000] vs. [91]\n\t [[Node: Equal = Equal[T=DT_INT64, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](ArgMax, ArgMax_1)]]\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "batch_size = 256\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "loss_buffer = list()\n",
    "\n",
    "for e in range(epochs):\n",
    "    \n",
    "    for X_batch, Y_batch in get_batches(trainset, batch_size):\n",
    "            \n",
    "        loss_value, _ = sess.run([loss, optimizer], feed_dict={\n",
    "            inputs: X_batch,\n",
    "            keep_prob: 1,\n",
    "            targets: Y_batch\n",
    "        })\n",
    "        \n",
    "        loss_buffer.append(loss_value)        \n",
    "        print(loss_value)\n",
    "        \n",
    "        \n",
    "        print(\"Working on accuracy...\")\n",
    "        #Check accuracy\n",
    "        acc_list = list()\n",
    "        for X_valid, Y_valid in get_batches(validset, 1000):\n",
    "            \n",
    "            print(\"Calculating acc...\")\n",
    "            \n",
    "            accuracy_value = sess.run(accuracy, feed_dict={\n",
    "                inputs: X_valid,\n",
    "                keep_prob: 1,\n",
    "                targets: Y_valid\n",
    "            })\n",
    "            \n",
    "            acc_list.append(accuracy_value)\n",
    "            \n",
    "        valid_accuracy = sum(acc_list) / len(acc_list)     \n",
    "        print(valid_accuracy)\n",
    "\n",
    "    #acc_value = sess.run([accuracy], feed_dict={\n",
    "        #inputs: X_valid,\n",
    "        #targets: y_valid\n",
    "    #})\n",
    "    \n",
    "    #print(e, loss_value, acc_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
