{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/iolucas/dlnd-projects/blob/master/image-classification/dlnd_image_classification.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow V1.9.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow V{}\".format(tf.__version__))\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from data_manager import load_dataset\n",
    "\n",
    "import json\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "import random\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrokesDataset:\n",
    "    def __init__(self, db_addr=\"localhost\"):\n",
    "        #Connect to db\n",
    "        self.conn = psycopg2.connect(\"dbname='postgres' user='postgres' host='{}' password='lucas'\".format(db_addr))\n",
    "        self.cur = self.conn.cursor()\n",
    "        \n",
    "        #Retrieve and cache ids list to use to query data later\n",
    "        self.cur.execute(\"SELECT id FROM samples LIMIT 220000\")\n",
    "        id_list = self.cur.fetchall()\n",
    "        self.id_cache = [id_value[0] for id_value in id_list]\n",
    "        \n",
    "        #Lets save key_to_ind to ensure the labels will be in the same order all the time\n",
    "        #Try to load key_to_ind, if not succeed, get it and save it\n",
    "        try:\n",
    "            key_list = pickle.load(open(\"key_list.pickle\", \"rb\"))\n",
    "            #self.key_to_ind = pickle.load(open(\"key_to_ind.pickle\", \"rb\"))\n",
    "            print(\"Loaded key_list file.\")\n",
    "        except:\n",
    "            #Retrieve and cache labels\n",
    "            self.cur.execute(\"SELECT DISTINCT key FROM samples LIMIT 300000\")\n",
    "            key_list = [key_value[0] for key_value in self.cur.fetchall()]\n",
    "            pickle.dump(key_list, open(\"key_list.pickle\", \"wb\"), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            print(\"Generated new key_list file.\")\n",
    "            \n",
    "        self.key_to_ind = dict([(key, ind) for ind, key in enumerate(key_list)])\n",
    "        self.ind_to_key = key_list\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "    def get_train_valid_test_ids(self):\n",
    "        train_ids, valid_ids = train_test_split(self.id_cache, test_size=0.4)\n",
    "        valid_ids, test_ids = train_test_split(valid_ids, test_size=0.5)\n",
    "        return tuple(train_ids), tuple(valid_ids), tuple(test_ids)\n",
    "    \n",
    "    def DEPRECTED_get_batches(self, ids, batch_size):\n",
    "        assert batch_size < 5000\n",
    "        \n",
    "        self.cur.execute(\"SELECT key, strokes FROM samples WHERE id in {}\".format(ids))\n",
    "       \n",
    "        while True:\n",
    "            data_batch = self.cur.fetchmany(batch_size)\n",
    "            if len(data_batch) == 0:\n",
    "                break\n",
    "                \n",
    "            ziped = list(zip(*data_batch)) #Use zip to separate labels from datapoints\n",
    "            \n",
    "            yield ziped[1], ziped[0]\n",
    "            \n",
    "    def get_batches(self, ids, batch_size):\n",
    "        assert batch_size < 5000\n",
    "        \n",
    "        for i in range(0,len(ids), batch_size):        \n",
    "            self.cur.execute(\"SELECT key, strokes FROM samples WHERE id in {}\".format(ids[i:i+batch_size]))\n",
    "       \n",
    "            data_batch = self.cur.fetchall()\n",
    "                \n",
    "            ziped = list(zip(*data_batch)) #Use zip to separate labels from datapoints\n",
    "            \n",
    "            yield ziped[1], ziped[0]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_image(strokes_list):\n",
    "    im = Image.new(mode=\"1\", size=(500,500))\n",
    "\n",
    "    draw = ImageDraw.Draw(im)\n",
    "    #draw.moveTo(strokes[0][:2], fill=128)\n",
    "    #print(strokes)\n",
    "    #draw.line((0, 0) + im.size, fill=128)\n",
    "    #draw.line((0, im.size[1], im.size[0], 0), fill=128)\n",
    "    for strokes in strokes_list:\n",
    "        draw.line(strokes, fill=128, width=5)\n",
    "    \n",
    "    \n",
    "    #for stroke in strokes[0:]:\n",
    "        #print(stroke.tolist())\n",
    "        #draw.line(stroke, fill=128, width=10)\n",
    "\n",
    "    plt.imshow(im)\n",
    "\n",
    "# write to stdout\n",
    "#im.save(sys.stdout, \"PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_tuples(strokes_list):\n",
    "    strokes_group = list()\n",
    "    for strokes in strokes_list:\n",
    "        strokes_tuples = list()\n",
    "        for x,y,t in strokes:\n",
    "            strokes_tuples.append((x,y))\n",
    "        strokes_group.append(strokes_tuples)\n",
    "    return strokes_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_random_tuple(size):\n",
    "    assert size < 100000\n",
    "    rand_list = list()\n",
    "    while len(rand_list) < size:\n",
    "        rand_value = random.randint(1,50000)\n",
    "        if rand_value not in rand_list:\n",
    "            rand_list.append(rand_value)\n",
    "        \n",
    "    return tuple(rand_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded key_list file.\n"
     ]
    }
   ],
   "source": [
    "ds = StrokesDataset(\"localhost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1097"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.key_to_ind[ds.ind_to_key[1097]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "symbols = json.loads(open(\"../../dataset/detexify data/symbols.json\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\sum $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gen_tuples(test_batch[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-6a317a13cd4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'sum'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mdraw_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_tuples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_batch' is not defined"
     ]
    }
   ],
   "source": [
    "for i, b in enumerate(test_batch[1]):\n",
    "    if 'sum' in b:\n",
    "        draw_image(gen_tuples(test_batch[0][i]))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, validset, testset = ds.get_train_valid_test_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "dataset_tuples = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 320 ms\n"
     ]
    }
   ],
   "source": [
    "%time test_batch = next(ds.get_batches(trainset,1000))\n",
    "for i in range(1000):\n",
    "    #draw_image(gen_tuples(test_batch[0][i]))\n",
    "    dataset_tuples.append(gen_tuples(test_batch[0][i]))\n",
    "    #print(test_batch[1][59])\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(dataset_tuples, open( \"save.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 94, 139,   0,   1,   0],\n",
       "       [  1,   0,   1,   0,   0],\n",
       "       [  7,   0,   1,   0,   0],\n",
       "       ...,\n",
       "       [ -2,   2,   1,   0,   0],\n",
       "       [  0,   1,   1,   0,   0],\n",
       "       [  0,   1,   0,   0,   1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform_data(strokes_list):\n",
    "    \"\"\"Convert strokes to match (deltaX, deltaY, is_drawing, is_moving, is_finished)\"\"\"\n",
    "    \n",
    "    onehots = list() #Gen pen moving onehots\n",
    "    flatten_strokes = [[0,0,0]] #Reshape strokes lists\n",
    "    \n",
    "    for strokes in strokes_list:\n",
    "        strokes = sorted(strokes, key=lambda a: a.__getitem__(2), reverse=False) #ensure strokes are in order\n",
    "        flatten_strokes += strokes\n",
    "        strokes_onehots = [[0,1,0]] + [[1,0,0]]*(len(strokes)-1)\n",
    "        onehots += strokes_onehots\n",
    "       \n",
    "    #Set end flag at the last coordinate\n",
    "    onehots[-1] = [0,0,1]\n",
    "        \n",
    "    flatten_strokes = np.array(flatten_strokes)\n",
    "    delta_strokes = flatten_strokes[1:,:2] - flatten_strokes[:-1,:2] #Get delta values (:2 removes the timing data)\n",
    "    \n",
    "    np_batch = np.concatenate((delta_strokes, onehots), axis=1)\n",
    "    \n",
    "    return np_batch.astype(int)\n",
    "    \n",
    "%time transform_data(test_batch[0][600])\n",
    "#draw_image(gen_tuples(test_batch[0][600]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/tutorials/recurrent_quickdraw<br>\n",
    "https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell<br>\n",
    "https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/DropoutWrapper<br>\n",
    "https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell<br>\n",
    "https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/stack_bidirectional_dynamic_rnn<br>\n",
    "https://github.com/tensorflow/models/blob/master/tutorials/rnn/quickdraw/train_model.py<br>\n",
    "https://github.com/udacity/deep-learning/blob/master/intro-to-rnns/Anna_KaRNNa_Solution.ipynb<br>\n",
    "https://github.com/tensorflow/magenta/tree/master/magenta/models/sketch_rnn<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNModel():\n",
    "    def __init__(self, n_classes, n_lstm_layers=2, n_lstm_nodes=128, grad_clip=9.0, learning_rate=0.01, dropout=0.0, cudnn=False):      \n",
    "        \n",
    "        with tf.variable_scope(\"rnn_model\", reuse=tf.AUTO_REUSE):\n",
    "            \n",
    "            #Placeholders\n",
    "            inputs = tf.placeholder(tf.float32, shape=[None, None, 5], name=\"inputs\")\n",
    "            #keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "            targets = tf.placeholder(tf.int32, shape=[None], name=\"targets\")\n",
    "            targets_onehot = tf.one_hot(targets, n_classes)\n",
    "            \n",
    "            \n",
    "            if cudnn:\n",
    "                #Cudnn LSTM\n",
    "                #CudnnLSTM is time-major, so we need to transpose\n",
    "                inputs_t = tf.transpose(inputs, [1, 0, 2])\n",
    "\n",
    "                lstm = tf.contrib.cudnn_rnn.CudnnLSTM(\n",
    "                    num_layers=n_lstm_layers,\n",
    "                    num_units=n_lstm_nodes,\n",
    "                    dropout=dropout,\n",
    "                    direction=\"bidirectional\")\n",
    "\n",
    "                lstm_outputs_t, _ = lstm(inputs_t)\n",
    "\n",
    "                # Convert back from time-major outputs to batch-major outputs.\n",
    "                lstm_outputs = tf.transpose(lstm_outputs_t, [1, 0, 2])\n",
    "            else:\n",
    "                \n",
    "                def create_rnn_cell(lstm_size, keep_prob):\n",
    "                    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "                    cell = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob=keep_prob)\n",
    "                    return cell\n",
    "                \n",
    "                fw_cells = [create_rnn_cell(n_lstm_nodes, 1 - dropout) for _ in range(n_lstm_layers)] #Forwards cells\n",
    "                bw_cells = [create_rnn_cell(n_lstm_nodes, 1 - dropout) for _ in range(n_lstm_layers)] #Backwards cells\n",
    "                \n",
    "                lstm_outputs, _, _ = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n",
    "                    fw_cells,\n",
    "                    bw_cells,\n",
    "                    inputs,\n",
    "                    dtype=tf.float32\n",
    "                )\n",
    "\n",
    "        \n",
    "        \n",
    "            #Outputs\n",
    "            logits = tf.layers.dense(lstm_outputs[:,-1,:], n_classes)\n",
    "\n",
    "            outputs = tf.nn.softmax(logits)\n",
    "\n",
    "            loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=targets_onehot, logits=logits)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "\n",
    "            #Create optimizers\n",
    "            # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "            tvars = tf.trainable_variables()\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "            train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "            optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "            # Accuracy\n",
    "            correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(targets_onehot, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "            \n",
    "            #Get references\n",
    "            self.accuracy = accuracy\n",
    "            self.inputs = inputs\n",
    "            self.targets = targets\n",
    "            self.loss = loss\n",
    "            self.optimizer = optimizer\n",
    "            self.outputs = outputs\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-38-119837663943>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-38-119837663943>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    check whether beta values of training are being create twice\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#tf.reset_default_graph()\n",
    "#RNNModel(10)\n",
    "check whether beta values of training are being create twice\n",
    "go aws fast!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dir(tf.GraphKeys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.global_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _add_cudnn_rnn_layers(convolved, num_nodes, num_layers, keep_prob):\n",
    "    \"\"\"Adds CUDNN LSTM layers.\"\"\"\n",
    "    # Convolutions output [B, L, Ch], while CudnnLSTM is time-major.\n",
    "    \n",
    "    convolved = tf.transpose(convolved, [1, 0, 2])\n",
    "    lstm = tf.contrib.cudnn_rnn.CudnnLSTM(\n",
    "        num_layers=num_layers,\n",
    "        num_units=num_nodes,\n",
    "        dropout=keep_prob,\n",
    "        direction=\"bidirectional\")\n",
    "    outputs, _ = lstm(convolved)\n",
    "    # Convert back from time-major outputs to batch-major outputs.\n",
    "    outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#Create network\n",
    "#lstm_size = 128\n",
    "#num_layers = 2\n",
    "#grad_clip = 9.0\n",
    "#learning_rate = 0.01\n",
    "n_classes = len(ds.key_to_ind)\n",
    "\n",
    "#tf.reset_default_graph()\n",
    "train_rnn_model = RNNModel(n_classes=n_classes, dropout=0.0)\n",
    "infer_rnn_model = RNNModel(n_classes=n_classes, dropout=0.0)\n",
    "\n",
    "#inputs = tf.placeholder(tf.float32, shape=[None, None, 5], name=\"inputs\")\n",
    "#keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "\n",
    "#targets = tf.placeholder(tf.int32, shape=[None], name=\"targets\")\n",
    "\n",
    "#targets_onehot = tf.one_hot(targets, n_classes)\n",
    "\n",
    "\n",
    "#rnn_outputs = _add_cudnn_rnn_layers(inputs, lstm_size, num_layers, keep_prob)\n",
    "\n",
    "\n",
    "#fw_cells = [create_rnn_cell(lstm_size, keep_prob)] #Forwards cells\n",
    "#bw_cells = [create_rnn_cell(lstm_size, keep_prob)] #Backwards cells\n",
    "\n",
    "#rnn_outputs, output_state_fw, output_state_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n",
    "    #fw_cells, bw_cells, inputs, dtype=tf.float32)\n",
    "\n",
    "#Use [:,-1,:] to keep only the (batches, last outputs, classes)\n",
    "#logits = tf.layers.dense(rnn_outputs[:,-1,:], n_classes)\n",
    "\n",
    "#outputs = tf.nn.softmax(logits)\n",
    "\n",
    "#loss = tf.nn.softmax_cross_entropy_with_logits(labels=targets_onehot, logits=logits)\n",
    "#loss = tf.reduce_mean(loss)\n",
    "\n",
    "#Create optimizers\n",
    "#optimizer = tf.train.AdamOptimizer(0.01).minimize(cost)\n",
    "\n",
    "# Optimizer for training, using gradient clipping to control exploding gradients\n",
    "#tvars = tf.trainable_variables()\n",
    "#grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "#train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "#optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "# Accuracy\n",
    "#correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(targets_onehot, 1))\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_batches(dataset, batch_size):\n",
    "    for x_batch, y_batch in ds.get_batches(dataset, batch_size):\n",
    "        #Transform X data to get delta cordinates, \n",
    "        trans_x = list(map(transform_data, x_batch))\n",
    "        \n",
    "        #make them the same size of the max size        \n",
    "        max_size = max(map(len, trans_x))\n",
    "        x_data_buffer = np.array([0,0,0,0,1]) * np.ones((batch_size, max_size, 5)).astype(int)\n",
    "        for i, x_transf_data in enumerate(trans_x):\n",
    "            x_data_buffer[i][:len(x_transf_data)] = x_transf_data\n",
    "        \n",
    "        #transform y data to get indexes for each label\n",
    "        yield x_data_buffer, list(map(ds.key_to_ind.get, y_batch))\n",
    " \n",
    "#Test\n",
    "#%time testdata = next(get_batches(trainset, 5))\n",
    "#print(testdata[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%time draw_image(gen_tuples(next(test_batches)[0][8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126272"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "493.25"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "126272/256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41984"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.floor(len(validset)/256)*256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded key_list file.\n",
      "7.001681\n",
      "6.8799314\n",
      "6.449214\n",
      "6.635103\n",
      "6.313327\n",
      "6.273525\n",
      "6.2732987\n",
      "6.2320237\n",
      "6.227172\n",
      "6.2402124\n",
      "Working on accuracy...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n",
      "Calculating acc...\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "\n",
    "train_batch_size = 256\n",
    "valid_batch_size = 256\n",
    "\n",
    "max_trainset_len = math.floor(len(trainset)/train_batch_size) * train_batch_size\n",
    "max_validset_len = math.floor(len(validset)/valid_batch_size) * valid_batch_size\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "ds = StrokesDataset(\"189.62.80.32\")\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "loss_buffer = list()\n",
    "\n",
    "loss = train_rnn_model.loss\n",
    "optimizer = train_rnn_model.optimizer\n",
    "t_inputs = train_rnn_model.inputs\n",
    "t_targets = train_rnn_model.targets\n",
    "\n",
    "accuracy = infer_rnn_model.accuracy\n",
    "a_inputs = infer_rnn_model.inputs\n",
    "a_targets = infer_rnn_model.targets\n",
    "\n",
    "for e in range(epochs):\n",
    "    \n",
    "    batch_iter = 0\n",
    "    \n",
    "    for X_batch, Y_batch in get_batches(trainset[:max_trainset_len], train_batch_size):\n",
    "        batch_iter += 1\n",
    "            \n",
    "        loss_value, _ = sess.run([loss, optimizer], feed_dict={\n",
    "            t_inputs: X_batch,\n",
    "            t_targets: Y_batch\n",
    "        })\n",
    "        \n",
    "        loss_buffer.append(loss_value)        \n",
    "        print(loss_value)\n",
    "        \n",
    "        \n",
    "        if batch_iter % 10 == 0:\n",
    "        \n",
    "            print(\"Working on accuracy...\")\n",
    "            #Check accuracy\n",
    "            acc_list = list()\n",
    "            for X_valid, Y_valid in get_batches(validset[:max_validset_len], valid_batch_size):\n",
    "\n",
    "                print(\"Calculating acc...\")\n",
    "\n",
    "                accuracy_value = sess.run(accuracy, feed_dict={\n",
    "                    a_inputs: X_valid,\n",
    "                    a_targets: Y_valid\n",
    "                })\n",
    "\n",
    "                acc_list.append(accuracy_value)\n",
    "\n",
    "            valid_accuracy = sum(acc_list) / len(acc_list)     \n",
    "            print(valid_accuracy)\n",
    "\n",
    "    #acc_value = sess.run([accuracy], feed_dict={\n",
    "        #inputs: X_valid,\n",
    "        #targets: y_valid\n",
    "    #})\n",
    "    \n",
    "    #print(e, loss_value, acc_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
